{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Copyright (c) 2022 Huawei Technologies Co., Ltd.<br>\n", "Licensed under CC BY-NC-SA 4.0 (Attribution-NonCommercial-ShareAlike 4.0 International) (the \"License\");<br>\n", "you may not use this file except in compliance with the License.<br>\n", "You may obtain a copy of the License at<br>\n", "<br>\n", "    https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode<br>\n", "<br>\n", "The code is released for academic research use only. For commercial use, please contact Huawei Technologies Co., Ltd.<br>\n", "Unless required by applicable law or agreed to in writing, software<br>\n", "distributed under the License is distributed on an \"AS IS\" BASIS,<br>\n", "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br>\n", "See the License for the specific language governing permissions and<br>\n", "limitations under the License.<br>\n", "<br>\n", "This repository was forked from https://github.com/openai/guided-diffusion, which is under the MIT license"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Various utilities for neural networks.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import math"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch as th\n", "import torch.nn as nn"]}, {"cell_type": "markdown", "metadata": {}, "source": ["PyTorch 1.7 has SiLU, but we support PyTorch 1.5."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SiLU(nn.Module):\n", "    def forward(self, x):\n", "        return x * th.sigmoid(x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class GroupNorm32(nn.GroupNorm):\n", "    def forward(self, x):\n", "        return super().forward(x.float()).type(x.dtype)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def conv_nd(dims, *args, **kwargs):\n", "    \"\"\"\n", "    Create a 1D, 2D, or 3D convolution module.\n", "    \"\"\"\n", "    if dims == 1:\n", "        return nn.Conv1d(*args, **kwargs)\n", "    elif dims == 2:\n", "        return nn.Conv2d(*args, **kwargs)\n", "    elif dims == 3:\n", "        return nn.Conv3d(*args, **kwargs)\n", "    raise ValueError(f\"unsupported dimensions: {dims}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def linear(*args, **kwargs):\n", "    \"\"\"\n", "    Create a linear module.\n", "    \"\"\"\n", "    return nn.Linear(*args, **kwargs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def avg_pool_nd(dims, *args, **kwargs):\n", "    \"\"\"\n", "    Create a 1D, 2D, or 3D average pooling module.\n", "    \"\"\"\n", "    if dims == 1:\n", "        return nn.AvgPool1d(*args, **kwargs)\n", "    elif dims == 2:\n", "        return nn.AvgPool2d(*args, **kwargs)\n", "    elif dims == 3:\n", "        return nn.AvgPool3d(*args, **kwargs)\n", "    raise ValueError(f\"unsupported dimensions: {dims}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def update_ema(target_params, source_params, rate=0.99):\n", "    \"\"\"\n", "    Update target parameters to be closer to those of source parameters using\n", "    an exponential moving average.\n", "    :param target_params: the target parameter sequence.\n", "    :param source_params: the source parameter sequence.\n", "    :param rate: the EMA rate (closer to 1 means slower).\n", "    \"\"\"\n", "    for targ, src in zip(target_params, source_params):\n", "        targ.detach().mul_(rate).add_(src, alpha=1 - rate)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def zero_module(module):\n", "    \"\"\"\n", "    Zero out the parameters of a module and return it.\n", "    \"\"\"\n", "    for p in module.parameters():\n", "        p.detach().zero_()\n", "    return module"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def scale_module(module, scale):\n", "    \"\"\"\n", "    Scale the parameters of a module and return it.\n", "    \"\"\"\n", "    for p in module.parameters():\n", "        p.detach().mul_(scale)\n", "    return module"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def mean_flat(tensor):\n", "    \"\"\"\n", "    Take the mean over all non-batch dimensions.\n", "    \"\"\"\n", "    return tensor.mean(dim=list(range(1, len(tensor.shape))))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def normalization(channels):\n", "    \"\"\"\n", "    Make a standard normalization layer.\n", "    :param channels: number of input channels.\n", "    :return: an nn.Module for normalization.\n", "    \"\"\"\n", "    return GroupNorm32(32, channels)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def timestep_embedding(timesteps, dim, max_period=10000):\n", "    \"\"\"\n", "    Create sinusoidal timestep embeddings.\n", "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n", "                      These may be fractional.\n", "    :param dim: the dimension of the output.\n", "    :param max_period: controls the minimum frequency of the embeddings.\n", "    :return: an [N x dim] Tensor of positional embeddings.\n", "    \"\"\"\n", "    half = dim // 2\n", "    freqs = th.exp(\n", "        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32) / half\n", "    ).to(device=timesteps.device)\n", "    args = timesteps[:, None].float() * freqs[None]\n", "    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n", "    if dim % 2:\n", "        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n", "    return embedding"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def checkpoint(func, inputs, params, flag):\n", "    \"\"\"\n", "    Evaluate a function without caching intermediate activations, allowing for\n", "    reduced memory at the expense of extra compute in the backward pass.\n", "    :param func: the function to evaluate.\n", "    :param inputs: the argument sequence to pass to `func`.\n", "    :param params: a sequence of parameters `func` depends on but does not\n", "                   explicitly take as arguments.\n", "    :param flag: if False, disable gradient checkpointing.\n", "    \"\"\"\n", "    if flag:\n", "        args = tuple(inputs) + tuple(params)\n", "        return CheckpointFunction.apply(func, len(inputs), *args)\n", "    else:\n", "        return func(*inputs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class CheckpointFunction(th.autograd.Function):\n", "    @staticmethod\n", "    def forward(ctx, run_function, length, *args):\n", "        ctx.run_function = run_function\n", "        ctx.input_tensors = list(args[:length])\n", "        ctx.input_params = list(args[length:])\n", "        with th.no_grad():\n", "            output_tensors = ctx.run_function(*ctx.input_tensors)\n", "        return output_tensors\n", "    @staticmethod\n", "    def backward(ctx, *output_grads):\n", "        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n", "        with th.enable_grad():\n", "            # Fixes a bug where the first op in run_function modifies the\n", "            # Tensor storage in place, which is not allowed for detach()'d\n", "            # Tensors.\n", "            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n", "            output_tensors = ctx.run_function(*shallow_copies)\n", "        input_grads = th.autograd.grad(\n", "            output_tensors,\n", "            ctx.input_tensors + ctx.input_params,\n", "            output_grads,\n", "            allow_unused=True,\n", "        )\n", "        del ctx.input_tensors\n", "        del ctx.input_params\n", "        del output_tensors\n", "        return (None, None) + input_grads"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}